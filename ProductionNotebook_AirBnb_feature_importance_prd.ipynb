{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirBnb Feature Importance Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Traveling to a nice touristic cosmopolitan city is different to travel to a small city or to a not so turistic one. Thats clear in our minds and there is no doubt about it. But could we tell the most important characteristics of an AirBnb acomodation that differ between the most visited cities and the not so visited ones. <br>\n",
    "\n",
    "With that said, the main motivation of the project is translated into the following research questions:\n",
    "\n",
    "### Research Questions\n",
    "1. Which are the 10 most important characteristics to differentiate the touristic cities and the not touristic ones?\n",
    "2. How does each of the 10 most important characteristics behaves in each touristic context of the cities where they are.\n",
    "3. How would be the most likely description of a acomodation in a city like NYC based on the findings of this study?\n",
    "4. How would be the most likely description of a acomodation in a small city like Salem, Oregon based on the findings of this study?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\DTI\n",
      "[nltk_data]     Digital\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import re\n",
    "from decimal import Decimal\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toolkit functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonnumeric_chars(s):\n",
    "    '''\n",
    "    INPUT:\n",
    "    s - numeric string with non-numeric charachters\n",
    "\n",
    "    OUTPUT:\n",
    "    np.float16 - numeric value before the dot\n",
    "    '''\n",
    "    try:\n",
    "        return np.float64(Decimal(re.sub(r'[^\\d.]', '', s)))\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_df(df, cat_cols, dummy_na, drop_first=True):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with categorical variables you want to dummy\n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    \n",
    "    OUTPUT:\n",
    "    df - a new dataframe that has the following characteristics:\n",
    "            1. contains all columns that were not specified as categorical\n",
    "            2. removes all the original columns in cat_cols\n",
    "            3. dummy columns for each of the categorical columns in cat_cols\n",
    "            4. if dummy_na is True - it also contains dummy columns for the NaN values\n",
    "            5. Use a prefix of the column name with an underscore (_) for separating \n",
    "    '''\n",
    "    \n",
    "    cat_df_out = pd.get_dummies(df[cat_cols], prefix=cat_cols, prefix_sep='_', dummy_na=dummy_na, drop_first=drop_first)\n",
    "    df_out     = cat_df_out.join(df.drop(cat_cols, axis=1))\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(vector, split_val):\n",
    "    ''''\n",
    "    INPUT:\n",
    "    vector - raw serie (pd.Series)\n",
    "    split_val - reference str to split the series\n",
    "\n",
    "    OUTPUT:\n",
    "    median, mean, max, min values of the sequences found in the serie\n",
    "    '''    \n",
    "    sequences = ''.join([str(val) for val in vector]).split(split_val)\n",
    "    sequences_count = [len(val) for val in sequences]\n",
    "    sequences_cleansed = [val for val in sequences_count if val>0]\n",
    "\n",
    "    if len(sequences_cleansed) > 0:\n",
    "        return np.median(sequences_cleansed), np.mean(sequences_cleansed), np.max(sequences_cleansed), np.min(sequences_cleansed)\n",
    "    else:\n",
    "        return 0,0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(data, feature, title=None ,rows=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    data - dataframe based on which the plot will be done\n",
    "    feature - feature to plot\n",
    "    title - plot title, default=None\n",
    "    rows - number of rows to show\n",
    "    '''\n",
    "    corr_abs = data.iloc[:rows]\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(5,7), dpi=200)#, sharey=True)\n",
    "    gs = fig.add_gridspec(1, 3)\n",
    "\n",
    "    sns.heatmap(corr_abs, annot=True, cmap='magma_r', cbar=False,ax=ax[0])\n",
    "    sns.barplot(x=feature, y=corr_abs.index, data=corr_abs, palette=\"magma\")\n",
    "\n",
    "    ax[0].axes.get_xaxis().set_visible(False)\n",
    "    ax[1].axes.get_xaxis().set_visible(False)\n",
    "    ax[1].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    sns.despine(bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL\n",
    "\n",
    "### tourism data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and load tourism data\n",
    "tourism = pd.read_csv('datasets/raw/city_tourism.csv',sep=';')\n",
    "# making sure to cleanse the names\n",
    "for col in ['city','city_name','state','region','country']:\n",
    "    tourism[col] = tourism[col].apply(lambda x: x.strip())\n",
    "# preparing the cities names to iterate through the cities\n",
    "cities = tourism['city'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calendar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [45:38<00:00, 119.05s/it]  \n"
     ]
    }
   ],
   "source": [
    "trusted_calendar_dataset = pd.DataFrame()\n",
    "\n",
    "for city in tqdm(cities):\n",
    "    # defining path to read data\n",
    "    calendar_path = f'datasets/raw/{city}/calendar.csv.gz'\n",
    "\n",
    "    # extracting data\n",
    "    with gzip.open(calendar_path,'r') as f:\n",
    "        df_calendar = pd.read_csv(BytesIO(f.read()))\n",
    "\n",
    "    # dropping NaN values (not much missing values)\n",
    "    df_calendar.dropna(inplace=True)\n",
    "    \n",
    "    # object to numeric transformation\n",
    "    toNumeric_columns =  ['price','adjusted_price']\n",
    "    for col in toNumeric_columns:\n",
    "        df_calendar[col] = df_calendar[col].apply(remove_nonnumeric_chars)\n",
    "\n",
    "    # str to datetime transformation\n",
    "    df_calendar['date'] = pd.to_datetime(df_calendar['date'])\n",
    "\n",
    "    # dealing with categorical values\n",
    "    cat_cols_lst = ['available']\n",
    "    df_calendar = create_dummy_df(df_calendar, cat_cols_lst, dummy_na=False)\n",
    "\n",
    "    # Aggregating by listing_id\n",
    "    df_calendar_agg = df_calendar.groupby('listing_id').agg({'date':['min','max'],\n",
    "                                                        'price':['min', 'max', 'mean'], \n",
    "                                                        'minimum_nights':['min','max','mean'], \n",
    "                                                        'maximum_nights':['min','max'],\n",
    "                                                        'available_t':'sum'\n",
    "                                                        })  \n",
    "    \n",
    "    # Renaming aggregated columns\n",
    "    # concat agg column names\n",
    "    col_names = []\n",
    "    for cols in df_calendar_agg.columns:\n",
    "        new_col = '_'.join(cols)\n",
    "        col_names.append(new_col)\n",
    "\n",
    "    # rename columns\n",
    "    df_calendar_agg.columns = col_names\n",
    "\n",
    "\n",
    "    # Feature engineering\n",
    "    # number of days online\n",
    "    df_calendar_agg['online_days'] = (df_calendar_agg['date_max']-df_calendar_agg['date_min']).apply(lambda x: x.days)+1\n",
    "    df_calendar_agg.drop('date_max', axis=1, inplace=True)\n",
    "    df_calendar_agg.drop('date_min', axis=1, inplace=True)\n",
    "    # occupied days\n",
    "    df_calendar_agg['ocupied_days'] = df_calendar_agg['online_days'] - df_calendar_agg['available_t_sum']\n",
    "    # total revenue generated by the allocation\n",
    "    df_calendar_agg['revenue'] = df_calendar_agg['ocupied_days'] * df_calendar_agg['price_mean']\n",
    "    # ocupation rate\n",
    "    df_calendar_agg['ocupation_rate'] = df_calendar_agg['ocupied_days'] / df_calendar_agg['online_days']\n",
    "    # geographical and tourism data\n",
    "    df_calendar_agg['city'] = city\n",
    "    df_calendar_agg = df_calendar_agg.join(tourism.set_index('city'), on='city', how='left')\n",
    "    df_calendar_agg['foreign_visitors'] = df_calendar_agg['foreign_visitors'].astype('str').apply(lambda x: re.sub('\\.', '', x))\n",
    "\n",
    "    # calculating relative price based on each city\n",
    "    df_calendar_agg['price_mean_rel'] = df_calendar_agg['price_mean'] / df_calendar_agg['price_mean'].mean()\n",
    "    df_calendar_agg['price_min_rel'] = df_calendar_agg['price_min'] / df_calendar_agg['price_min'].mean()\n",
    "    df_calendar_agg['price_max_rel'] = df_calendar_agg['price_max'] / df_calendar_agg['price_max'].mean()\n",
    "\n",
    "    # calculating ocupation statistics\n",
    "    d = {'listing_id':[],\n",
    "        'ocupation_duration_median':[],\n",
    "        'ocupation_duration_mean':[],\n",
    "        'ocupation_duration_max':[],\n",
    "        'ocupation_duration_min':[]\n",
    "        }\n",
    "    # loop through each acomodation to impute the statistics\n",
    "    for listing_id in df_calendar_agg.index:\n",
    "        vector = df_calendar.loc[df_calendar['listing_id']==listing_id]['available_t']\n",
    "        median_val, mean_val, max_val, min_val = get_sequences(vector, '1')\n",
    "        d['listing_id'].append(listing_id) \n",
    "        d['ocupation_duration_median'].append(median_val)\n",
    "        d['ocupation_duration_mean'].append(mean_val)\n",
    "        d['ocupation_duration_max'].append(max_val)\n",
    "        d['ocupation_duration_min'].append(min_val)\n",
    "    # join with calculated statistics\n",
    "    ocupation_duration_stats = pd.DataFrame(d)\n",
    "    df_calendar_agg = df_calendar_agg.join(ocupation_duration_stats.set_index('listing_id'))   \n",
    "\n",
    "    # concat data to the calendar dataset\n",
    "    trusted_calendar_dataset = pd.concat([trusted_calendar_dataset, df_calendar_agg])\n",
    "\n",
    "\n",
    "# loading data to the trusted directory\n",
    "trusted_calendar_dataset.to_csv('datasets/trusted/calendar_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### listing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      "  4%|▍         | 1/23 [00:00<00:06,  3.31it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      "  9%|▊         | 2/23 [00:01<00:14,  1.43it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 13%|█▎        | 3/23 [00:01<00:10,  1.92it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 17%|█▋        | 4/23 [00:02<00:10,  1.84it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 22%|██▏       | 5/23 [00:02<00:07,  2.42it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 26%|██▌       | 6/23 [00:02<00:07,  2.40it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 30%|███       | 7/23 [00:03<00:10,  1.48it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 35%|███▍      | 8/23 [00:04<00:10,  1.37it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 39%|███▉      | 9/23 [00:05<00:11,  1.20it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 43%|████▎     | 10/23 [00:08<00:19,  1.51s/it]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 48%|████▊     | 11/23 [00:09<00:13,  1.16s/it]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 52%|█████▏    | 12/23 [00:09<00:11,  1.01s/it]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 57%|█████▋    | 13/23 [00:10<00:09,  1.11it/s]C:\\Users\\DTI Digital\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (67) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 61%|██████    | 14/23 [00:13<00:14,  1.56s/it]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 65%|██████▌   | 15/23 [00:13<00:09,  1.14s/it]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 70%|██████▉   | 16/23 [00:14<00:06,  1.14it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 74%|███████▍  | 17/23 [00:14<00:04,  1.35it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 78%|███████▊  | 18/23 [00:14<00:03,  1.55it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      "C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 87%|████████▋ | 20/23 [00:15<00:01,  1.98it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 91%|█████████▏| 21/23 [00:16<00:01,  1.93it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      " 96%|█████████▌| 22/23 [00:16<00:00,  1.99it/s]C:\\Users\\DTIDIG~1\\AppData\\Local\\Temp/ipykernel_35020/1375416946.py:58: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_listing = df_listing.fillna(df_listing.median())\n",
      "100%|██████████| 23/23 [00:17<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating empty dataset for listings data\n",
    "trusted_listing_dataset = pd.DataFrame()\n",
    "\n",
    "for city in tqdm(cities):\n",
    "    # defining path to read data\n",
    "    listing_path = f'datasets/raw/{city}/listings.csv.gz'\n",
    "\n",
    "    with gzip.open(listing_path,'r') as f:\n",
    "        df_listing = pd.read_csv(BytesIO(f.read()))\n",
    "\n",
    "    missing_cols_100 = df_listing.columns[df_listing.isna().mean()==1]\n",
    "    df_listing.drop(missing_cols_100, axis=1, inplace=True)\n",
    "\n",
    "    # for the ones below 5%, dropping the rows\n",
    "    to_drop_rows = df_listing.columns[(df_listing.isna().mean()<0.1)]\n",
    "    df_listing.dropna(subset=to_drop_rows, inplace=True)\n",
    "\n",
    "    object_cols = df_listing.columns[df_listing.dtypes=='object']\n",
    "\n",
    "    # cleansing host data\n",
    "    to_drop = ['host_url', 'host_name', 'host_location', 'host_about', 'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood', 'host_verifications']\n",
    "    df_listing.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # dropping not needed columns\n",
    "    to_drop = ['listing_url', 'name', 'description', 'picture_url', 'neighbourhood', 'amenities', 'price', 'has_availability',\n",
    "           'neighborhood_overview','calendar_last_scraped','last_review',\n",
    "           'bathrooms_text']\n",
    "    df_listing.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # dropping not needed columns\n",
    "    to_drop = ['scrape_id', 'host_id', 'host_listings_count', 'neighbourhood_cleansed', 'latitude', 'longitude', 'minimum_nights',\n",
    "            'maximum_nights','minimum_minimum_nights','maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights','minimum_nights_avg_ntm','maximum_nights_avg_ntm',\n",
    "            'availability_30','availability_60','availability_365','availability_90','number_of_reviews_ltm','calculated_host_listings_count','calculated_host_listings_count_entire_homes',\n",
    "            'calculated_host_listings_count_shared_rooms','calculated_host_listings_count_private_rooms', 'last_scraped']\n",
    "    df_listing.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # object to numeric transformation\n",
    "    toNumeric_columns =  ['host_response_rate','host_acceptance_rate']\n",
    "    for col in toNumeric_columns:\n",
    "        df_listing[col] = df_listing[col].apply(remove_nonnumeric_chars)\n",
    "    \n",
    "    # dealing with categorical columns\n",
    "    categorical_cols = ['host_response_time', 'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'room_type', 'instant_bookable']\n",
    "    df_listing = create_dummy_df(df_listing, categorical_cols, dummy_na=True)\n",
    "    \n",
    "    # list of date columns\n",
    "    date_cols = ['host_since','first_review']\n",
    "    # str to datetime transformation\n",
    "    for col in date_cols:\n",
    "        df_listing[col] = pd.to_datetime(df_listing[col])\n",
    "    # transforming the date into the number of days until the last date\n",
    "    for col in date_cols:\n",
    "        df_listing[col] = (df_listing[col].max()-df_listing[col]).apply(lambda x: x.days)\n",
    "\n",
    "    # imputing the mean value to the missing values\n",
    "    df_listing['first_review'] = df_listing['first_review'].fillna(np.mean(df_listing['first_review']))\n",
    "    # imputing the median value to the missing values\n",
    "    df_listing = df_listing.fillna(df_listing.median())\n",
    "\n",
    "    # concat datasets\n",
    "    trusted_listing_dataset = pd.concat([trusted_listing_dataset, df_listing])\n",
    "\n",
    "# drop features that are not always found in axis\n",
    "for col in ['license','neighbourhood_group_cleansed', 'property_type']:\n",
    "    try:\n",
    "        trusted_listing_dataset.drop(col, axis=1, inplace=True)\n",
    "    except:\n",
    "        print(f'{col} not found in axis')\n",
    "\n",
    "# making sure that no NaN column is encountered in the dataset\n",
    "trusted_listing_dataset = trusted_listing_dataset.fillna(trusted_listing_dataset.median())\n",
    "    \n",
    "# loading data to the trusted directory\n",
    "trusted_listing_dataset.to_csv('datasets/trusted/listing_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 20/23 [57:25<06:37, 132.44s/it]  "
     ]
    }
   ],
   "source": [
    "# creating empty dataset for the review data\n",
    "trusted_reviews_dataset = pd.DataFrame()\n",
    "\n",
    "for city in tqdm(cities):\n",
    "    # defining path to read data\n",
    "    reviews_path = f'datasets/raw/{city}/reviews.csv.gz'\n",
    "\n",
    "    with gzip.open(reviews_path,'r') as f:\n",
    "        df_reviews = pd.read_csv(BytesIO(f.read()))\n",
    "\n",
    "    # dropping nan values\n",
    "    df_reviews.dropna(inplace=True)\n",
    "    # dropping useless columns\n",
    "    df_reviews.drop(['id', 'reviewer_id',\t'reviewer_name'], axis=1, inplace=True)\n",
    "\n",
    "    # performing sentiment analysis on the comments given\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    df_reviews[\"review_comment_score\"] = df_reviews['comments'].apply(sid.polarity_scores)\n",
    "    df_reviews[\"review_comment_score\"] = df_reviews[\"review_comment_score\"].apply(lambda review: review['compound'])\n",
    "\n",
    "    # transforming the date into the number of days until the last date\n",
    "    df_reviews['date'] = pd.to_datetime(df_reviews['date'])\n",
    "    df_reviews['review_date'] = (df_reviews['date'].max()-df_reviews['date']).apply(lambda x: x.days)\n",
    "\n",
    "    # dropping the transformed columns\n",
    "    df_reviews.drop(['date','comments'], axis=1, inplace=True)\n",
    "\n",
    "    # Grouping by listing_id\n",
    "    df_reviews_agg = df_reviews.groupby('listing_id').agg({'review_comment_score':['min','max','mean','median'],\n",
    "                                                       'review_date':['min', 'max', 'mean','median'], \n",
    "                                                       })\n",
    "    # concat agg column names\n",
    "    col_names = []\n",
    "    for cols in df_reviews_agg.columns:\n",
    "        new_col = '_'.join(cols)\n",
    "        col_names.append(new_col)\n",
    "    # rename columns\n",
    "    df_reviews_agg.columns = col_names\n",
    "\n",
    "    # concat data to the reviews dataset\n",
    "    trusted_reviews_dataset = pd.concat([trusted_reviews_dataset, df_reviews_agg])\n",
    "\n",
    "# loading data to the trusted directory\n",
    "trusted_reviews_dataset.to_csv('datasets/trusted/reviews_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the data\n",
    "calendar_path = 'datasets/trusted/calendar_dataset.csv'\n",
    "listing_path = 'datasets/trusted/listing_dataset.csv'\n",
    "reviews_path = 'datasets/trusted/reviews_dataset.csv'\n",
    "\n",
    "calendar_trusted = pd.read_csv(calendar_path)\n",
    "listing_trusted = pd.read_csv(listing_path)\n",
    "reviews_trusted = pd.read_csv(reviews_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining all the raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined = reviews_trusted.set_index('listing_id')\\\n",
    "                            .join(listing_trusted.set_index('id'), how='left') \\\n",
    "                            .join(calendar_trusted.set_index('listing_id'), how='left')\n",
    "# drop nan values before persisting dataset\n",
    "df_refined.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the outliars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = 50\n",
    "df_refined['host_acceptance_rate'] = df_refined['host_acceptance_rate'].apply(lambda x: x if x>=min_val else min_val)\n",
    "\n",
    "max_val = 3\n",
    "df_refined['price_max_rel'] = df_refined['price_max_rel'].apply(lambda x: x if x<=max_val else max_val)\n",
    "\n",
    "max_val = 1000\n",
    "df_refined['price_max'] = df_refined['price_max'].apply(lambda x: x if x<=max_val else max_val)\n",
    "\n",
    "max_val = 100\n",
    "df_refined['minimum_nights_min'] = df_refined['minimum_nights_min'].apply(lambda x: x if x<=max_val else max_val)\n",
    "\n",
    "max_val = 3\n",
    "df_refined['price_min_rel'] = df_refined['price_min_rel'].apply(lambda x: x if x<=max_val else max_val)\n",
    "\n",
    "max_val = 30\n",
    "df_refined['host_total_listings_count'] = df_refined['host_total_listings_count'].apply(lambda x: x if x<=max_val else max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "df_refined.to_csv('datasets/refined/refined_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the refined dataset\n",
    "df_refined = pd.read_csv('datasets/refined/refined_dataset.csv')\n",
    "# Loading the most important features for the XGBoost model\n",
    "most_important_XGBoost = pd.read_csv('datasets/refined/most_important_XGBoost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing the most important features plus the target feature\n",
    "most_important_features = list(most_important_XGBoost.iloc[:10].index) + ['foreign_visitors']\n",
    "# creating a copy of the dataset in memory to be used on the trainig process\n",
    "df_refined_feed_model = df_refined[most_important_features].copy()\n",
    "\n",
    "# using the standard scaler to standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# separating the train and target features\n",
    "X = df_refined_feed_model[most_important_features].drop('foreign_visitors', axis=1)\n",
    "y = df_refined_feed_model['foreign_visitors']\n",
    "\n",
    "# train and test datasets split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# standardization of the dataset\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# check the shape of the train and test datasets\n",
    "print(f'features X_train: {len(X_train[1])}\\nfeatures X_test: {len(X_test[1])}')\n",
    "\n",
    "# Learning - XGBoost\n",
    "xgbc = XGBRegressor()\n",
    "xgbc.fit(X_train, y_train)\n",
    "xgbc_pred_train=xgbc.predict(X_train)\n",
    "xgbc_pred_test=xgbc.predict(X_test)\n",
    "\n",
    "# performing crossvalidation test \n",
    "xgscore = np.mean(cross_val_score(estimator=xgbc,\n",
    "                       X=X_train, y=y_train, cv=5))\n",
    "# printing cross validation results\n",
    "print(f'XGBoost cross val score: {xgscore}')\n",
    "\n",
    "# analysing train and test data results\n",
    "train_score_xgbc =  r2_score(y_train, xgbc_pred_train)\n",
    "test_score_xgbc = r2_score(y_test, xgbc_pred_test)\n",
    "# printing r2 scores of training and test\n",
    "print(f'test score: {test_score_xgbc} \\n train score: {train_score_xgbc}')\n",
    "\n",
    "# getting the most important features\n",
    "dict_importance = xgbc.get_booster().get_score(importance_type=\"total_gain\")\n",
    "d={}\n",
    "# cleanse the data\n",
    "for index, value in dict_importance.items():\n",
    "    d[X.columns[int(index[1:])]] = value\n",
    "# transforming data to a dataframe\n",
    "most_important_XGBoost = pd.DataFrame(d, index=['Total gain']).T\n",
    "most_important_XGBoost = most_important_XGBoost[['Total gain']].sort_values(ascending=False, by='Total gain')\n",
    "\n",
    "# plot most important features\n",
    "plot_importance(most_important_XGBoost, 'Total gain', 'XGBoost Model [Total gain]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Question 1\n",
    "\n",
    "The 10 most important characteristics to differentiate the listings of an AirBnb Acomodation are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature tourism level to further answer the questions\n",
    "df_refined['tourism_level'] = pd.qcut(df_refined['foreign_visitors'], q=3, labels=['low', 'medium', 'high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='host_acceptance_rate', y='tourism_level', data=df_refined, palette='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acceptance rate in the less visited cities is higher than from the most visited ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='price_max_rel', y='tourism_level', data=df_refined, palette='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum price in the most visited cities varies less then the ones from the less visited cities, which show a higher variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='price_max', y='tourism_level', data=df_refined, palette='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum price in the most visited cities varies less then the ones from the less visited cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='minimum_nights_min', y='tourism_level', data=df_refined, palette='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be more acomodations with a higher minimum nights min in the most visited cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='price_min_rel', y='tourism_level', data=df_refined, palette='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general the minimum relative prices in high visited cities seem to be slightly lower than the ones of the less visited cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='host_response_time_nan', y='tourism_level', data=df_refined, palette='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a higher likelihood that the host do not respond in the most visited cities, which generates a NaN value for the response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='host_total_listings_count', y='tourism_level', data=df_refined, palette='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems that the owners of acomodations in the most visited cities own less acomodations in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='ocupation_duration_median', y='tourism_level', data=df_refined, palette='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tourists seem to stay longer in the most visited cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined[most_important_features + ['tourism_level']].groupby('tourism_level').agg(['max','mean','median','min']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2b72ad74ec8b1368a383d0f41ddd77f160c566387e4279ab43f9a0692ac185e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
